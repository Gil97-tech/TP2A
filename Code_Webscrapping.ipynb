{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRAPPING DES DONNEES SUR LES LOGEMENTS DE AIRBNB PARIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le but de cette partie est de scraper les données sur l'ensemble des logements porposés par airbnb Paris du 30 Décembre 2020 au 31 Décembre 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from selenium import webdriver\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import time        \n",
    "import requests  \n",
    "import re          \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from joblib import Parallel, delayed \n",
    "import itertools\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.action_chains import ActionChains "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la première étape consiste à recupérer le code HTLML de la page qui contient l'ensemble des logments à partir de son url. Cette page étant statique, BeautifulSoup sera suffisant.\n",
    "Par la suite, la classe des differents logements est récupéré. Cette classe est obtenue en inspectant le site web de airbnb Paris qui contient des annonces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPage(url):\n",
    "\tresult = requests.get(url)\n",
    "\tcontent = result.content\n",
    "\treturn BeautifulSoup(content, features = \"lxml\")\n",
    "\n",
    "def getRoomClasses(soupPage):\n",
    "\trooms = soupPage.findAll(\"div\", {\"class\": \"_8ssblpx\"})\n",
    "\tresult = []\n",
    "\tfor room in rooms:\n",
    "\t\tresult.append(room)\n",
    "\treturn result\n",
    "\n",
    "url_page =\"https://www.airbnb.fr/s/Paris--France/homes?adults=1&place_id=ChIJD7fiBh9u5kcRYJSMaMOCCwQ&refinement_paths%5B%5D=%2Fhomes&checkin=2020-12-30&checkout=2020-12-31\"\n",
    "page=getPage(url_page)\n",
    "listing=getRoomClasses(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l'objectif suivant est de récupérer un ensemble d'informations sur chaque logment notamment: le nombre de commentaires, l'évaluation, le lien vers le logement, le titre du logement, la description du logement, le nombre de chambre, de lit, de douche ainsi que le prix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##commentaires\n",
    "def getNbCommentaire(listing):\n",
    "\ttry:\n",
    "\t\treturn listing.find(\"span\", {\"class\":\"_a7a5sx\"}).text \n",
    "\texcept:\n",
    "\t\treturn \"pas de commentaire\"\n",
    "\n",
    "##evaluation\n",
    "def getEvaluation(listing):\n",
    "\ttry:\n",
    "\t\treturn listing.find(\"span\", {\"class\":\"_10fy1f8\"}).text \n",
    "\texcept:\n",
    "\t\treturn \"pas d'évaluation\"\n",
    "\n",
    "## lien du logement\n",
    "def getListingLink(listing):\n",
    "\ttry:\n",
    "\t\treturn \"http://airbnb.com\" + listing.find(\"a\")[\"href\"]\n",
    "\texcept:\n",
    "\t\treturn \"pas de lien\"\n",
    "\n",
    "## titre du logement\n",
    "def getListingTitle(listing):\n",
    "\ttry:\n",
    "\t\treturn listing.find(\"meta\")[\"content\"]\n",
    "\texcept:\n",
    "\t\treturn \"pas de titre du logement\"\n",
    "\n",
    "##description du logement\n",
    "def getTopRow(listing):\n",
    "\ttry:\n",
    "\t\treturn listing.find(\"div\", {\"class\": \"_167qordg\"}).text\n",
    "\texcept:\n",
    "\t\treturn \"pas de description du logement\"   \n",
    "    \n",
    "##information sur le nombre de chambre/douche\n",
    "def getRoomInfo(listing):\n",
    "\ttry:\n",
    "\t\treturn listing.find(\"div\", {\"class\":\"_kqh46o\"}).text\n",
    "\texcept:\n",
    "\t\treturn \"pas d'infos sur chambre/douche\"\n",
    "##prix\n",
    "def getPrix(listing):\n",
    "\ttry:\n",
    "\t\treturn listing.find(\"div\", {\"class\":\"_1fwiw8gv\"}).text\n",
    "\texcept:\n",
    "\t\treturn \"pas d'infos sur le prix\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’étape suivante consiste à scrapper la liste de tous les logments de Paris. Au lieu d’une seule page, Airbnb présente ses logements sur une quinzaine de pages. Au bas de chaque page, il est possible de cliquer sur un bouton pour passer à la page suivante. Notre objectif est d’automatiser la tâche d’aller à la page suivante. \n",
    "Nous commençons par identifier l’objet fléché. l'inspection de la page nous montre qu'il s'agit d'un objet de la classe « _za9j7e » de type « a ». \n",
    "Nous automatisons de ce fait, le passage à la page suivante et l'extraction de l'URL de chaque page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findNextPage(soupPage):\n",
    "\ttry:\n",
    "\t\tnextpage = \"https://airbnb.com\" + soupPage.find(\"a\", {\"class\": \"_za9j7e\"})[\"href\"]\n",
    "\texcept:\n",
    "\t\tnextpage = \"no next page\"\n",
    "\treturn nextpage\n",
    "\n",
    "def getPages(url):\n",
    "\tresult = []\n",
    "\twhile url != \"no next page\": \n",
    "\t\tpage = getPage(url)\n",
    "\t\tresult = result + [page]\n",
    "\t\turl = findNextPage(page)\n",
    "\treturn result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le code suivant permet de créer une base de données comportant toutes les informations que l'on desire scrapper sur l'ensemble des logements de Airbnp paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extraction des infos de tous les logements d'une page\n",
    "def extractinfo(page):\n",
    "    df = pd.DataFrame(columns = ['title', 'link',\"nbComments\",\"prix\",\"topRow\",\"evaluation\",\"roomInfo\"]) ##,\"evaluation\", ,'nbComments'\n",
    "    new=[]\n",
    "    listing=getRoomClasses(page)\n",
    "    for i in range(1, len(listing)):\n",
    "        new.append(getListingTitle(listing[i]))\n",
    "        new.append(getListingLink(listing[i]))\n",
    "        new.append(getNbCommentaire(listing[i]))\n",
    "        new.append(getPrix(listing[i]))\n",
    "        new.append(getTopRow(listing[i]))\n",
    "        new.append(getEvaluation(listing[i]))\n",
    "        new.append(getRoomInfo(listing[i]))\n",
    "        df.loc[i]=new\n",
    "        new=[]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping proprement dit des informations sur l'ensemble des logements présents dans l'annonce de Airbnb Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scrapper toutes les infos sur les logements\n",
    "def extractPages(url):\n",
    "\tpages = getPages(url)\n",
    "\tdf =extractinfo(pages[0])\n",
    "\tfor pagenumber in range(1, len(pages)):\n",
    "\t\tdf = df.append(extractinfo(pages[pagenumber]))\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##base de tous les logements avec les évaluations\n",
    "data=pd.DataFrame.from_dict(extractPages(url_page))\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour éviter de lancer l'ensemble des codes à chaque fois, nous décidons d'exporter la première partie de notre jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.index)\n",
    "import csv\n",
    "data.to_csv('D:\\Python\\dataListing.csv', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrapping des commentaires de chaque logement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour scrapper les commentaires de chaque logement, il incombe d'ouvrir individuellement chacun d'eux. Les liens vers la page détaillée de chaque logement obtunus lors du premier crapping sont utilisés. \n",
    "Malheureusement, notre méthode précédente de scrapping d’un site web statique ne peut pas être utilisée car Airbnb charge le contenu de ces pages détaillées sur la page Web à l’aide de javascript. Nous utilisons donc dans la suite la package *Selenium*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le pilote Web utilisé est Chrome. l'étape suivante est donc de configurer ce pilote. On définit également un temps d'attente necessaire au chargement de chaque page. Nous avons défini un temps d'attente de 5 secondes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## installer chrome driver\n",
    "driver_path = 'D:/chromedriver.exe'\n",
    "opt = webdriver.ChromeOptions()\n",
    "opt.add_experimental_option('w3c', False)\n",
    "driver = webdriver.Chrome(executable_path=driver_path,options=opt)\n",
    "\n",
    "##Initialiser le driver de Selenium\n",
    "def setupDriver(url, waiting_time = 5):\n",
    "\tdriver = webdriver.Chrome(options=opt)\n",
    "\tdriver.get(url)\n",
    "\ttime.sleep(waiting_time) \n",
    "\treturn driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La page de chaque logement permet d'avoir les commentaires en cliquant sur un lien. ce lien nous dirige vers une autre page dans laquelle l'extraction effective a lieu.\n",
    "Le code suivant permet de charger chaque page detaillée et recupérer le lien de la page des commentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## charger la page de chaque logement\n",
    "def getJSpage(url):\n",
    "\tdriver = setupDriver(url)\n",
    "\thtml = driver.page_source\n",
    "\tdriver.close()\n",
    "\treturn BeautifulSoup(html, features=\"lxml\")\n",
    "\n",
    "# lien de la page des commentaires de chaque listing\n",
    "def getCommentLink(soupPage):\n",
    "\ttry:\n",
    "\t\treturn \"https://www.airbnb.fr\" + soupPage.find(\"div\", {\"class\": \"_19qg1ru\"}).find(\"a\")[\"href\"]\n",
    "\texcept:\n",
    "\t\treturn \"pas de lien\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois la page des commentaires ouverte, il incombe de déterminer la classe qui sera utilisée pour l'extraction. l'inspection de la page revèle qu'il s'agit d'un objet de classe \"_1gjypya\". Dans cette objet, sont présents les commentaires des clients ainsi que ceux de Airbnb. Nous allons nous restreindre aux seuls commentaires des clients en utilisant la classe \"_1y6fhhr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Recuperer l'ensemble des commentaires\n",
    "def getRoomComments(soupage):\n",
    "    #try\n",
    "    crooms=soupage.findAll('div', {'class': '_1gjypya'})\n",
    "    result=[]\n",
    "    for room in crooms:\n",
    "        result.append(room)\n",
    "    return result\n",
    "\n",
    "def getcomments(result):\n",
    "    comments=[]\n",
    "    for i in result:\n",
    "        try:\n",
    "            comments.append(i.find('div', {'class': '_1y6fhhr'}).text)\n",
    "        except: \n",
    "            pass\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Précisons que l'ensemble des commentaires d'un logement s'ouvre dans une boite de dialogue qu'il faudra au préalable faire défiler ou scroller complètement avant l'extraction. le code suivant permet donc de scroller la boite de dialogue des commentaires et recupèrer le code source de la page.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Scrolling\n",
    "def ScrollPagecomments(url):\n",
    "    driver=setupDriver(url)\n",
    "    num_current_comment_found = 0\n",
    "    pre_scroll_num_of_comment=-1\n",
    "    comment_class_when_scrolling  = '_1gjypya'\n",
    "    time.sleep(5)\n",
    "    while (num_current_comment_found != pre_scroll_num_of_comment) :           \n",
    "                visible_comments = driver.find_elements_by_class_name(comment_class_when_scrolling)\n",
    "                pre_scroll_num_of_comment= len(visible_comments)\n",
    "                try:\n",
    "                    last_visible_comment = visible_comments[-1]\n",
    "                    actions = ActionChains(driver)\n",
    "                    actions.move_to_element(last_visible_comment)\n",
    "                    actions.perform()\n",
    "                    time.sleep(2)\n",
    "                    last_visible_comment.location_once_scrolled_into_view\n",
    "                    visible_comments = driver.find_elements_by_class_name(comment_class_when_scrolling)\n",
    "                    num_current_comment_found = len(visible_comments)\n",
    "                except:\n",
    "                    pass\n",
    "    html=driver.page_source\n",
    "    driver.close()\n",
    "    return BeautifulSoup(html,features=\"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code suivant met ensemble les fonctions intermediaires précedemment présentées afin de constituer une base de données qui contient pour chaque logement l'ensemble des commentaires. Elle prend en entrée la base de données obtenue à l'issue du premier scrapping. La base est sauvegardée au fur et à mesure que l'extraction s'effectue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractionCommentaire(data):\n",
    "    ind=[]\n",
    "    com=[]\n",
    "    DataComments=[]\n",
    "    i=0\n",
    "    for link in data['link']:\n",
    "        soupPage=getJSpage(link)\n",
    "        url_comment=getCommentLink(soupPage)\n",
    "        if (url_comment!=\"pas de lien\"):\n",
    "            js=ScrollPagecomments(url_comment)\n",
    "            DataComments=getcomments(getRoomComments(js))\n",
    "            #a=getcomments(getRoomComments(js))\n",
    "        else :\n",
    "            DataComments=[\"pas de commentaire\"]\n",
    "        i=i+1\n",
    "        ind=ind+list(itertools.repeat(i, len(DataComments)))\n",
    "        com=com + DataComments\n",
    "        database=pd.DataFrame.from_dict({'index':ind, 'commentaires':com})\n",
    "        database.to_csv('D:\\Python\\database.csv', index = False, encoding = 'utf-8')\n",
    "    return database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette base de commentaires est utilisée dans la suite pour faire du NLP (Natural Language Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExtractionCommentaire(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
